# -*- coding: utf-8 -*-
"""task_4_ds_intern.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jlOxA-PeJ-fZ6rejq3vBHvwNAAOH7QMV

*TASK 4*

*Analyze and visualize sentiment patterns in social media data to understand public opinion and attitudes towards specific topics or brands.*
"""

!pip install tensorflow

import pandas as pd
import numpy as np
import re
import string
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout,SpatialDropout1D,SimpleRNN,LSTM
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')
import matplotlib.pyplot as plt
import seaborn as sns

"""**LOADED TWITTER SENTIMENT ANALYSIS TRAINING DATASET**"""

train_data=pd.read_csv('twitter_training.csv')
train_data.head()

"""**EXPLORATORY DATA ANALYSIS**"""

df=pd.read_csv('twitter_training.csv')
counts_sentiment=df['sentiment'].value_counts()
plt.figure(figsize=[5,5])
sns.countplot(x=df['sentiment'],palette='Accent',order=['Positive','Negative','Neutral','Irrelevant'])
plt.title('Count Analysis of Tweet Sentiments')
for index,Counts in enumerate(counts_sentiment):
  plt.text(index,Counts,str(Counts),color='black',va='baseline',ha='center')
plt.show()

values=df['sentiment'].value_counts()
senti_labels=['Positive','Negative','Neutral','Irrelevant']

ordered_val=[values.get(i,0)for i in senti_labels]
pie_palette=sns.color_palette('Greens',len(senti_labels))
plt.pie(ordered_val,labels=senti_labels,colors=pie_palette,autopct='%.0f%%')
plt.title('Sentiment Distribution of Tweets')
plt.show()

"""**REQUIRED VARIABLES**"""

train_data=train_data[['tweet_content','sentiment']]

"""**SHAPE OF TRAINING DATASET**"""

print('Shape of train dataset: ',train_data.shape)

"""**DROPPING NULL VALUES**"""

train_data=train_data.dropna()

"""**SHAPE OF TRAINING DATASET AFTER DROPPING NULL VALUES**"""

print('Shape of train dataset: ',train_data.shape)

print('Train dataset','\n')

print(train_data.head())

"""**COUNT OF SENTIMENTS IN TRANING AND VALIDATION DATASET**"""

print('Counts of Sentiments in train dataset','\n')

print(train_data['sentiment'].value_counts(),'\n')

"""**REMOVING 'irrelevant' LABELLED SENTIMENT**"""

train_data=train_data[train_data['sentiment'] != 'Irrelevant']

print('counts of sentiments in train dataset after removing "Irrelevant" value','\n')
print(train_data['sentiment'].value_counts(),'\n')

"""**SHAPE OF TRAIN DATASET AFTER REMOVING 'irrelevant' LABELLED SENTIMENTS**"""

print('Shape of train dataset: ',train_data.shape)

"""**CONVERTING TWEETS INTO LOWERCASE, REMOVING EXTRA SPACES AND PUNCTUATION**"""

def preprocess_text(text):
    text = text.lower()  # Lowercase
    text = re.sub(f"[{re.escape(string.punctuation)}]", "", text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra spaces
    return text

train_data['tweet_content'] = train_data['tweet_content'].apply(preprocess_text)

"""**APPLYING TOKENIZATION, SEQUENCING, AND PADDING PREPROCESSING TECHNIQUES ON TWEETS**"""

# Tokenize the text
max_words = 5000
max_len = 100

tokenizer = Tokenizer(num_words=max_words)
tokenizer.fit_on_texts(train_data['tweet_content'])

X= tokenizer.texts_to_sequences(train_data['tweet_content'])

X= pad_sequences(X, maxlen=max_len)

"""**ENCODING SENTIMENTS**"""

# Encode the labels
encoder = LabelEncoder()
Y= encoder.fit_transform(train_data['sentiment'])

# Convert labels to categorical one-hot encoding
Y = to_categorical(Y, num_classes=3)

print(X)


print(Y)

"""**SPLITTING DATASET INTO TRAIN AND TEST**"""

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)
print(X_train.shape,Y_train.shape)
print(X_test.shape,Y_test.shape)

"""**BUILDING GRU MODEL FOR SENTIMENT ANALYSIS**"""

model = Sequential()
model.add(Embedding(input_dim=max_words, output_dim=128, input_length=X_train.shape[1]))
model.add(SpatialDropout1D(0.2))
model.add(GRU(units=128,recurrent_dropout=0.2))
model.add(Dropout(0.2))
model.add(Dense(3, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

"""**TRAINING THE GRU MODEL**"""

history = model.fit(X_train,Y_train, epochs=3, batch_size=128,validation_data=(X_test, Y_test),verbose = 2)

"""**EVALUATION OF GRU MODEL**"""

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(Y_test, axis=1)

"""**VISUALIZING ACCURACY AND LOSS OF GRU**"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'b', label='Training acc',linestyle='dashed')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'orange', label='Training loss',ls='dashed')
plt.plot(epochs, val_loss, 'orange', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""**CLASSIFICATION REPORT OF GRU**"""

print(classification_report(y_test_classes, y_pred_classes))
print(f"Accuracy: {accuracy_score(y_test_classes, y_pred_classes)}")

"""#*COMPARING GRU PERFORMANCE WITH SimpleRNN*#

**BUILDING SimpleRNN MODEL FOR SENTIMENT ANALYSIS**
"""

model_1 = Sequential()
model_1.add(Embedding(input_dim=max_words, output_dim=128, input_length=X_train.shape[1]))
model_1.add(SpatialDropout1D(0.2))
model_1.add(SimpleRNN(units=128,recurrent_dropout=0.2))
model_1.add(Dropout(0.2))
model_1.add(Dense(3, activation='softmax'))

model_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_1.summary()

"""**TRAINING THE SimpleRNN MODEL**"""

history_1= model_1.fit(X_train, Y_train, epochs=3, batch_size=128,validation_data=(X_test,Y_test),verbose=2)

"""**EVALUATION OF SimpleRNN MODEL**"""

y_pred_1= model_1.predict(X_test)
y_pred_classes_1= np.argmax(y_pred_1, axis=1)
y_test_classes_1= np.argmax(Y_test, axis=1)

"""**VISUALIZING ACCURACY AND LOSS OF RNN**"""

acc = history_1.history['accuracy']
val_acc = history_1.history['val_accuracy']
loss = history_1.history['loss']
val_loss = history_1.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'g', label='Training acc',linestyle='dashed')
plt.plot(epochs, val_acc, 'g', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'r', label='Training loss',ls='dashed')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""**CLASSIFICATION REPORT OF SimpleRNN**"""

print(classification_report(y_test_classes_1, y_pred_classes_1))
print(f"Accuracy: {accuracy_score(y_test_classes_1, y_pred_classes_1)}")

"""#*COMPARING GRU PERFORMANCE WITH LSTM*#

**BUILDING LSTM MODEL FOR SENTIMENT ANALYSIS**
"""

model_2 = Sequential()
model_2.add(Embedding(input_dim=max_words, output_dim=128, input_length=X_train.shape[1]))
model_2.add(SpatialDropout1D(0.2))
model_2.add(LSTM(units=128,recurrent_dropout=0.2))
model_2.add(Dropout(0.2))
model_2.add(Dense(3, activation='softmax'))

model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model_2.summary()

"""**TRAINING THE LSTM MODEL**"""

history_2=model_2.fit(X_train, Y_train, epochs = 3, batch_size=128, validation_data=(X_test, Y_test),verbose = 2)

"""**EVALUATION OF LSTM MODEL**"""

y_pred_2= model_2.predict(X_test)
y_pred_classes_2= np.argmax(y_pred_2, axis=1)
y_test_classes_2= np.argmax(Y_test, axis=1)

"""**VISUALIZING ACCURACY AND LOSS OF LSTM**"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'purple', label='Training acc',ls='dashed')
plt.plot(epochs, val_acc, 'purple', label='Validation acc')
plt.title('Training and validation accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'magenta', label='Training loss',ls='dashed')
plt.plot(epochs, val_loss, 'magenta', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()

"""**CLASSIFICATION REPORT OF LSTM**"""

print(classification_report(y_test_classes_2, y_pred_classes_2))
print(f"Accuracy: {accuracy_score(y_test_classes_2, y_pred_classes_2)}")

"""#**CONCLUSION**#

*LOSS and ACCURACY*


**LSTM ACHIEVED LOWEST LOSS VALUE & HIGHEST ACCURACY THAN GRU AND SimpleRNN**
"""